---\naliases:\n  - Computation Graph AD\n  - AD Graphs\n  - Dynamic vs Static Graphs\n---\n\n# Computation Graphs for AD\n\nComputation graphs are directed acyclic graphs (DAGs) that represent mathematical computations performed by a program, such as evaluating a function or a machine learning model. They are a central data structure used by many [[Automatic Differentiation in ML|Automatic Differentiation (AD)]] systems to organize and execute derivative calculations.\n\n## Structure\n\n- **Nodes**: Represent variables (inputs, parameters, constants, or intermediate values).\n- **Edges**: Represent elementary operations (e.g., +, \*, sin, exp, matrix multiplication) that transform input nodes into output nodes.\n\n**Example**: For the function $y = (a+b)*c$\n\n```mermaid\ngraph TD\n    A[a] --> Plus((+))\n    B[b] --> Plus\n    Plus --> Mul((*))\n    C[c] --> Mul\n    Mul --> Y[y]\n```\n\n## Role in Automatic Differentiation\n\nComputation graphs provide the structure needed for AD algorithms:\n\n1.  **[[Forward Primal Trace]]**: Evaluating the function corresponds to a forward traversal of the graph, computing the value at each node based on its inputs.\n2.  **[[Forward Mode Automatic Differentiation]]**: Augments the forward traversal by computing derivatives (tangents) alongside values at each node.\n3.  **[[Reverse Mode Automatic Differentiation]]**: First performs the forward primal trace (storing intermediate values and the graph structure), then performs a [[Reverse Adjoint Trace|backward traversal]] from the output node(s), applying the chain rule at each edge to compute derivatives (adjoints).\n\n## Static vs. Dynamic Computation Graphs\n\nThis is a key distinction between different ML frameworks and AD implementations:\n\n### Static Graphs\n\n- **Definition**: The graph structure is defined *declaratively* and *fixed* before any computation runs.\n- **Process**: Define Graph -> Compile Graph -> Execute Graph (potentially multiple times with different inputs).\n- **Examples**: TensorFlow 1.x (Graph Mode), Theano, [[JAX]] (when using `@jit`).\n- **Advantages**:\n    - **Optimization**: Allows for extensive graph-level optimizations (e.g., operation fusion, constant folding, memory planning) by the compiler ([[Just In Time Compilation]]) *before* execution.\n    - **Deployment**: Easier to serialize and deploy the optimized graph to various platforms.\n    - **Potential Performance**: Can achieve higher performance due to pre-compilation and optimization.\n- **Disadvantages**:\n    - **Less Flexibility**: Control flow (if/else, loops) dependent on input data can be cumbersome to express (requires special control flow ops like `tf.cond`, `tf.while_loop`, or `jax.lax.cond`, `jax.lax.scan`).\n    - **Debugging**: Debugging can be harder as errors might occur within the compiled graph, away from the Python code that defined it (though tools are improving).\n    - **Steeper Learning Curve**: The define-then-run paradigm can be less intuitive initially.\n\n### Dynamic Graphs\n\n- **Definition**: The graph structure is defined *imperatively* and *built on-the-fly* as the forward computation executes.\n- **Process**: Execute operations directly, implicitly creating the graph structure (often via operator overloading and a tape) as needed for potential backward passes.\n- **Examples**: PyTorch (default), TensorFlow (Eager Execution mode - default in TF 2.x), Chainer.\n- **Advantages**:\n    - **Flexibility & Intuitiveness**: Feels more like standard programming. Control flow using native Python constructs (if/else, loops) works naturally.\n    - **Easier Debugging**: Errors occur directly where the operation is called in Python. Standard Python debuggers can be used.\n    - **Easier to Learn**: The define-by-run paradigm aligns well with standard Python scripting.\n- **Disadvantages**:\n    - **Optimization Opportunities**: Fewer opportunities for global graph optimization before execution (though [[Just In Time Compilation|JIT compilers]] like TorchScript or `tf.function` aim to bridge this gap by tracing dynamic code to create static graphs).\n    - **Runtime Overhead**: Potential overhead from interpreting Python code and building the graph dynamically at each step (though often negligible for large ML models).\n    - **Deployment**: Can be slightly more complex to serialize and deploy compared to a fully static graph.\n\n## Conclusion\n\nComputation graphs are the backbone of modern [[Automatic Differentiation in ML|AD systems]]. The choice between static and dynamic graphs represents a trade-off between upfront optimization potential and runtime flexibility/ease-of-use. Frameworks like JAX lean heavily towards static graphs (via JIT compilation) for performance, while PyTorch prioritizes the flexibility of dynamic graphs, with both offering mechanisms (like `tf.function` or TorchScript) to capture dynamic code into more static, optimizable forms. 