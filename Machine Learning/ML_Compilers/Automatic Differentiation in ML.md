---\naliases:\n  - Autodiff\n  - Algorithmic Differentiation ML\n---\n\n# Automatic Differentiation in Machine Learning\n\n**Automatic Differentiation (AD)**, also known as *autodiff* or *algorithmic differentiation*, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. It is a cornerstone technology enabling the training of modern deep learning models via gradient-based optimization (like stochastic gradient descent).\n\n## Why AD is Crucial for ML\n\n- **Gradient Calculation**: Training neural networks requires calculating the gradient of a loss function with respect to potentially millions or billions of model parameters.\n- **Efficiency**: AD provides an efficient way to compute these gradients, far superior to symbolic differentiation (which can lead to expression swell) or numerical differentiation (which is approximate and slow).\n- **Complex Models**: AD handles the complex, nested functions typical of deep neural networks.\n\n## Core Concepts\n\nAD relies on the fact that any computer program executing a function, no matter how complex, is ultimately a sequence of elementary arithmetic operations (addition, multiplication, etc.) and elementary functions (sin, exp, log, etc.) for which derivatives are known. By applying the chain rule repeatedly to these elementary operations, the derivative of the overall function can be computed automatically and accurately.\n\n### Key AD Modes\n\n1.  **[[Forward Mode Automatic Differentiation]]**: Computes derivatives by propagating them forward through the computation graph along with the function evaluation. Efficient when the number of inputs is small compared to the number of outputs.\n2.  **[[Reverse Mode Automatic Differentiation]]**: Computes derivatives by propagating them backward through the computation graph after an initial forward pass computes the function value. This is the foundation of **backpropagation** in neural networks and is highly efficient when the number of outputs (typically a single scalar loss value) is small compared to the number of inputs (model parameters).\n\n### [[Computation Graphs for AD|Computation Graphs]]\n\nAD techniques often rely on representing the function computation as a graph:\n- **Nodes**: Represent variables or intermediate values.\n- **Edges**: Represent elementary operations.\nAD algorithms traverse this graph (either forward or backward) to accumulate derivatives.\n- **Dynamic Graphs** (e.g., PyTorch, TensorFlow Eager): The graph is built on-the-fly as the computation executes.\n- **Static Graphs** (e.g., TensorFlow 1.x, JAX with `@jit`): The graph is defined and compiled first, then executed.\n\n## AD in ML Frameworks\n\nModern ML frameworks heavily rely on AD:\n\n- **[[JAX]]**: Uses AD transformations (`jax.grad`) combined with [[Just In Time Compilation]] (`jax.jit`) and [[Vectorization and Parallelization|vectorization]] (`jax.vmap`, `jax.pmap`) on top of XLA.\n- **PyTorch**: Primarily uses reverse-mode AD with dynamic computation graphs (Autograd engine).\n- **TensorFlow**: Supports both eager execution (dynamic graphs) and graph mode (static graphs) with its `tf.GradientTape` API for AD.\n\n## Implementation Techniques\n\n1.  **Operator Overloading**: Redefining basic arithmetic operators (+, \*, sin, etc.) to handle derivative calculations alongside value calculations (often using dual numbers for forward mode or specialized tape structures for reverse mode).\n2.  **Source Code Transformation**: Automatically rewriting the source code of the function to include derivative computation steps.\n\n## Advantages over Other Methods\n\n- **Symbolic Differentiation**: Can lead to exponentially large symbolic expressions (\"expression swell\") and may be difficult to implement efficiently.\n- **Numerical Differentiation (Finite Differences)**: Introduces truncation and round-off errors, computationally expensive (requires multiple function evaluations per parameter).\n\n## Conclusion\n\nAutomatic differentiation is a fundamental enabling technology for modern machine learning, particularly deep learning. By leveraging the chain rule on computational graphs, it provides an exact and efficient method for computing the gradients necessary for training complex models. Understanding the principles of forward and reverse mode AD is key to appreciating how frameworks like JAX, PyTorch, and TensorFlow operate under the hood.\n\n---\n\n**References**:\n1. Baydin, A. G., Pearlmutter, B. A., Radul, A. A., & Siskind, J. M. (2018). Automatic differentiation in machine learning: a survey. *Journal of Machine Learning Research*, *18*, 1-43. [Link](https://www.jmlr.org/papers/v18/17-468.html)\n2. Griewank, A., & Walther, A. (2008). *Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation* (2nd ed.). SIAM. [Link](https://en.wikipedia.org/wiki/Automatic_differentiation#Further_reading)\n3. Wikipedia contributors. (2025, February 1). *Automatic differentiation*. Wikipedia. [https://en.wikipedia.org/wiki/Automatic_differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) 